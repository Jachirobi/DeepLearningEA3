<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">üåô Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt ‚ÄûDaten‚Äú unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angeh√§ngt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen m√∂glich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z.‚ÄØB.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollst√§ndigen, durch Leerzeichen
						getrennten W√∂rtern bestehen. Durch Klick auf den Button <em>‚ÄûVorhersage‚Äú</em>
						werden die wahrscheinlichsten folgenden W√∂rter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						W√∂rter ausw√§hlen, welches dann angeh√§ngt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>2)</strong> Mit dem Button <em>‚ÄûWeiter‚Äú</em> kann
						das wahrscheinlichste Wort automatisch angeh√§ngt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>3)</strong> √úber den Button <em>‚ÄûAuto‚Äú</em> k√∂nnen
						automatisch bis zu 10 W√∂rter vorhergesagt werden.</li>
					<li><strong>4)</strong> Dieser Vorgang kann mit dem Button <em>‚ÄûStopp‚Äú</em>
						unterbrochen werden.</li>
					<li><strong>5)</strong> Mit dem Button <em>‚ÄûReset‚Äú</em> werden
						sowohl der eingegebene Text als auch das Netzwerk zur√ºckgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Reset</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begr√ºnden Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte n√§chste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zus√§tzliches Ma√ü.</li>
					<li>Untersuchen Sie, ob sich die urspr√ºnglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						m√∂gliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">
					<button id="predictBtn">üîÆ Vorhersage</button>
					<button id="weiterBtn">‚û°Ô∏è Weiter</button>
					<button id="autoBtn">ü§ñ Auto</button>
					<button id="stopBtn">üõë Stopp</button>
					<button id="resetBtn">üîÑ Reset</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschl√§ge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingef√ºgte Vorschl√§ge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzl√§nge von 5
							Token. Dies erm√∂glicht es dem Modell, sprachliche Abh√§ngigkeiten
							√ºber einen gr√∂√üeren Bereich zu erfassen.</p>

						<p>An das Embedding schlie√üen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich l√§nger wurde. Die Erh√∂hung der Units hat
							zu einer Verbesserung des Trainingsergebnisses gef√ºhrt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,3 sowie ein recurrent Dropout von 0,2 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollst√§ndige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), w√§hrend die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repr√§sentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt √ºber eine dichte Softmax-Schicht, deren
							Dimension der Gr√∂√üe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							√ºber alle potenziell n√§chsten W√∂rter zur√ºck.</p>

						<p>F√ºr das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Gr√∂√üe von 64 √ºber 60
							Epochen hinweg trainiert. Die Anzahl der Epochen hat sich als am
							besten herausgestellt, weil loss und accuracy am dichtesten
							beieinander waren.</p>

						<p>Diese Architektur erm√∂glicht eine stabile, regulierte
							Modellierung sprachlicher Abh√§ngigkeiten mit verbesserter
							Generalisierungsf√§higkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>
					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						<p>Das trainierte LSTM-Sprachmodell speichert keine konkreten
							Trainingss√§tze, sondern lediglich statistische Muster in seinen
							Gewichtungen. Eine direkte Rekonstruktion der urspr√ºnglichen
							Trainingsdaten ist daher nicht m√∂glich.</p>

						<p>Allerdings kann es bei h√§ufigen oder pr√§gnanten
							Formulierungen, insbesondere bei kleinem Datensatz oder
							Overfitting, vorkommen, dass Teile des Trainingsmaterials
							sinngem√§√ü oder sogar w√∂rtlich reproduziert werden. In solchen
							F√§llen besteht ein gewisses Datenschutzrisiko, insbesondere wenn
							personenbezogene Daten im Training enthalten waren. Auch wenn das
							Risiko bei einem Modell mit begrenztem Vokabular (z.‚ÄØB. 5000
							W√∂rter) und normalisierten Daten gering ist, sollte man bei der
							Ver√∂ffentlichung des Modells auf m√∂gliche R√ºckschl√ºsse auf
							sensible Inhalte achten.</p></li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>F√ºr die Regression der gegebenen Funktion wurde ein
					Feedforward Neural Network (FFNN) mit konfigurierbarer Architektur
					eingesetzt (2 Hidden-Layer, je 100 Neuronen, ReLU-Aktivierung,
					Adam-Optimizer mit Lernrate 0.01 und Batch-Size 32). Die Anzahl der
					Datenpunkte (N) und die Rauschvarianz (V) wurden gem√§√ü Vorgabe
					flexibel gestaltet (hier typischerweise N = 100, V = 0.05). Dadurch
					konnten gezielt Auswirkungen von Datenmenge und Rauschst√§rke auf
					das Modellverhalten untersucht werden.</p>
				<p>
					Auf den <strong>unverrauschten Daten</strong> zeigte das Modell bei
					einer Epochenzahl von 1000 ein sehr gutes Fit-Verhalten (Train- und
					Test-MSE < 0.0001). Bei den <strong>verrauschten Daten</strong>
					f√ºhrte ein Bereich von ca. 100-300 Epochen zu einer ausgewogenen
					Generalisierung (Train-MSE ~0.04-0.06, Test-MSE ~0.06-0.08). Bei
					sehr hoher Epochenanzahl (ab ca. 15.000) konnte Overfitting
					deutlich beobachtet werden: der Train-Loss sinkt weiter (&lt;0.02),
					w√§hrend der Test-Loss ansteigt (&gt;0.12 bei 15.000 und &gt;0.32
					bei 20.000).
				</p>
				<p>√Ñnderungen an der Architektur (z.B. zus√§tzliche Layer oder
					mehr Neuronen) zeigten, dass komplexere Modelle tendenziell
					schneller zu Overfitting neigen, insbesondere bei kleinen
					Datens√§tzen oder hoher Rauschvarianz. Ebenso wirkt sich die Wahl
					der Aktivierungsfunktion auf die Lernstabilit√§t aus (ReLU erwies
					sich hier als robust). Die Erh√∂hung der Datenpunkte f√ºhrte hingegen
					zu einem schnelleren Best-Fit Verhalten und einer Angleichung
					zwischen Test- und Trainings-MSE sowie einem viel sp√§teren
					Overt-Fit.</p>
				<p>Insgesamt hat dieses Experiment sehr anschaulich
					verdeutlicht, wie Modellkomplexit√§t, Trainingsdauer, Datenmenge und
					Rauschlevel gemeinsam das Lern- und Generalisierungsverhalten eines
					neuronalen Netzes bestimmen.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der L√∂sung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow / Keras</strong>: Zum Erstellen,
						Trainieren und Speichern des LSTM-basierten Language Models
						(Sequential, LSTM, Embedding, Adam, EarlyStopping, etc.).</li>
					<li><strong>TensorFlow.js</strong>: Zum Konvertieren des
						Keras-Modells in ein browserkompatibles Format f√ºr die
						clientseitige Vorhersage.</li>
					<li><strong>NumPy</strong>: F√ºr die effiziente Verarbeitung
						von Trainingsdaten (z.‚ÄØB. Sequenzen, Labels, Arrays).</li>
					<li><strong>re (Regular Expressions)</strong>: Zum Aufbereiten
						und Tokenisieren des Textkorpus (z.‚ÄØB. Satzzeichen trennen).</li>
					<li><strong>HTML, CSS, JavaScript</strong>: Zum Aufbau der
						Web-Oberfl√§che inklusive Texteingabe, Vorhersageanzeige und
						Interaktion mit dem TensorFlow.js-Modell im Browser.</li>
				</ul>
				<p>Das Modell nutzt ein gestapeltes LSTM mit Dropout und
					recurrent_dropout, um Overfitting zu vermeiden. Die Trainingsdaten
					werden mithilfe eines festen Werts von 5 W√∂rtern verarbeitet. Das
					Modell wird mit EarlyStopping trainiert und in .keras und .tfjs
					exportiert. Im Frontend wird durch Padding und Begrenzung der
					Wortindizes sichergestellt, dass Eingabesequenzen modellkompatibel
					bleiben. Im Testskript wird zus√§tzlich die Top-k-Genauigkeit und
					Perplexity als Evaluationsma√ü berechnet.</p>

				<h3>2) Fachlich</h3>
				<p>
					Die vorliegende Anwendung basiert auf einem autoregressiven Modell
					zur <strong>n√§chsten Wortvorhersage</strong>, das mithilfe eines
					LSTM-Netzwerks realisiert wurde. Grundlage f√ºr das Training bildet
					ein deutschsprachiger Korpus, der aus dem <a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">Wortschatz-Korpus der Universit√§t Leipzig</a>
					stammt. Der Text wurde im Vorfeld bereinigt und tokenisiert, wobei
					insbesondere Satzzeichen separiert und Gro√ü-/Kleinschreibung
					erhalten blieben.
				</p>

				<p>
					Das Modell verarbeitet Wortsequenzen mit einer L√§nge von f√ºnf
					Tokens und sagt jeweils das darauffolgende Wort vorher. Es wurde
					mit zwei gestapelten LSTM-Schichten (je 200 Einheiten) trainiert
					und durch Dropout regularisiert. W√§hrend des Trainings wurde
					<code>EarlyStopping</code>
					verwendet, um √úberanpassung zu vermeiden. Als Optimierer kam
					<code>Adam</code>
					mit einer Lernrate von 0,001 zum Einsatz.
				</p>

				<p>Nach erfolgreichem Training wurde das Modell ins
					TensorFlow.js-Format konvertiert und in eine interaktive
					Weboberfl√§che integriert. Diese erlaubt es Nutzer:innen, eigene
					Texte einzugeben, automatische Wortvorschl√§ge zu generieren oder
					Schritt f√ºr Schritt Texte fortzusetzen. Die Vorschl√§ge basieren auf
					den wahrscheinlichsten n√§chsten W√∂rtern laut Modell ‚Äì inklusive
					Prozentwerten.</p>

				<p>
					Zur qualitativen Bewertung des Modells wurden neben der klassischen
					Genauigkeit auch <em>Top-k-Genauigkeiten</em> (k=1, 5, 10, 20, 100)
					und die <strong>Perplexity</strong> als Kennzahlen berechnet. Diese
					geben Aufschluss √ºber die F√§higkeit des Modells, plausible
					Folgew√∂rter im Sprachkontext zu w√§hlen.
				</p>
				<p>
					<strong>Quellen & verwendete Technologien:</strong><br>
					Wortschatz-Korpus der Universit√§t Leipzig<br> (<a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">wortschatz.uni-leipzig.de</a>)<br> TensorFlow
					und Keras ‚Äì f√ºr Modellarchitektur, Training und Export<br> (<a
						href="https://www.tensorflow.org/" target="_blank">tensorflow.org</a>)<br>

					TensorFlow.js ‚Äì f√ºr die Modellnutzung im Browser<br> (<a
						href="https://js.tensorflow.org" target="_blank">js.tensorflow.org</a>)<br>

					NumPy ‚Äì f√ºr Array-Manipulation im Trainingsprozess<br> (<a
						href="https://numpy.org" target="_blank">numpy.org</a>)<br>

					Regul√§re Ausdr√ºcke (re) ‚Äì zur Textvorverarbeitung in Python<br>
					(<a href="https://docs.python.org/3/library/re.html"
						target="_blank">python.org ‚Ä∫ re</a>)<br> Chart.js ‚Äì f√ºr
					Visualisierungen im Frontend (optional/erg√§nzend)<br> (<a
						href="https://www.chartjs.org" target="_blank">chartjs.org</a>)<br>

					HTML, CSS, JavaScript (ES6) ‚Äì f√ºr die Benutzeroberfl√§che,
					Eingabelogik und Eventsteuerung<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web"
						target="_blank">MDN Web Docs</a>)<br> Fetch API ‚Äì zum Laden
					des Tokenizer-Wortindex im Browser<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"
						target="_blank">MDN Fetch API</a>)<br> Google Fonts ‚Äì f√ºr
					typografische Gestaltung<br> (<a
						href="https://fonts.google.com" target="_blank">fonts.google.com</a>)<br>

					Vorlesungsfolien, √úbungen und Diskussionen im Rahmen der
					Lehrveranstaltung
				</p>

			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
