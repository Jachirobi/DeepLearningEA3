<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<link rel="stylesheet"
	href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">üåô Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt ‚ÄûDaten‚Äú unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angeh√§ngt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen m√∂glich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z.‚ÄØB.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollst√§ndigen, durch Leerzeichen
						getrennten W√∂rtern bestehen. Durch Klick auf den Button <em>‚ÄûVorhersage‚Äú</em>
						werden die wahrscheinlichsten folgenden W√∂rter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						W√∂rter ausw√§hlen, welches dann angeh√§ngt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>2)</strong> Mit dem Button <em>‚ÄûWeiter‚Äú</em> kann
						das wahrscheinlichste Wort automatisch angeh√§ngt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>3)</strong> √úber den Button <em>‚ÄûAuto‚Äú</em> k√∂nnen
						automatisch bis zu 10 W√∂rter vorhergesagt werden.</li>
					<li><strong>4)</strong> Dieser Vorgang kann mit dem Button <em>‚ÄûStopp‚Äú</em>
						unterbrochen werden.</li>
					<li><strong>5)</strong> Mit dem Button <em>‚ÄûReset‚Äú</em> werden
						sowohl der eingegebene Text als auch das Netzwerk zur√ºckgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Reset</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begr√ºnden Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte n√§chste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zus√§tzliches Ma√ü.</li>
					<li>Untersuchen Sie, ob sich die urspr√ºnglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						m√∂gliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">

					<button id="predictBtn">
						<i class="fas fa-magic"></i> Vorhersage
					</button>
					<button id="weiterBtn">
						<i class="fas fa-forward"></i> Weiter
					</button>
					<button id="autoBtn">
						<i class="fas fa-robot"></i> Auto
					</button>
					<button id="stopBtn">
						<i class="fas fa-hand-paper"></i> Stopp
					</button>
					<button id="resetBtn">
						<i class="fas fa-redo"></i> Reset
					</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschl√§ge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingef√ºgte Vorschl√§ge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzl√§nge von 5
							Token. Dies erm√∂glicht es dem Modell, sprachliche Abh√§ngigkeiten
							√ºber einen gr√∂√üeren Bereich zu erfassen.</p>

						<p>An das Embedding schlie√üen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich l√§nger wurde. Die Erh√∂hung der Units hat
							zu einer Verbesserung des Trainingsergebnisses gef√ºhrt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,2 sowie ein recurrent Dropout von 0,1 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollst√§ndige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), w√§hrend die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repr√§sentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt √ºber eine dichte Softmax-Schicht, deren
							Dimension der Gr√∂√üe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							√ºber alle potenziell n√§chsten W√∂rter zur√ºck.</p>

						<p>F√ºr das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Gr√∂√üe von 128 √ºber 9
							Epochen hinweg trainiert. Die Anzahl der Epochen hat wurde
							mittels early stoppings ermittelt.</p>

						<p>Diese Architektur erm√∂glicht eine stabile, regulierte
							Modellierung sprachlicher Abh√§ngigkeiten mit verbesserter
							Generalisierungsf√§higkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>

					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						<p>Das trainierte LSTM-Sprachmodell speichert keine konkreten
							Trainingss√§tze, sondern lediglich statistische Muster in seinen
							Gewichtungen. Eine direkte Rekonstruktion der urspr√ºnglichen
							Trainingsdaten ist daher nicht m√∂glich.</p>

						<p>Allerdings kann es bei h√§ufigen oder pr√§gnanten
							Formulierungen, insbesondere bei kleinem Datensatz oder
							Overfitting, vorkommen, dass Teile des Trainingsmaterials
							sinngem√§√ü oder sogar w√∂rtlich reproduziert werden. In solchen
							F√§llen besteht ein gewisses Datenschutzrisiko, insbesondere wenn
							personenbezogene Daten im Training enthalten waren. Auch wenn das
							Risiko bei einem Modell mit begrenztem Vokabular (z.‚ÄØB. 5000
							W√∂rter) und normalisierten Daten gering ist, sollte man bei der
							Ver√∂ffentlichung des Modells auf m√∂gliche R√ºckschl√ºsse auf
							sensible Inhalte achten.</p></li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Das trainierte LSTM-Modell zeigte eine grunds√§tzlich
					funktionierende Vorhersagef√§higkeit f√ºr h√§ufige Wortfolgen,
					insbesondere bei klaren syntaktischen Mustern.</p>
				<p>Bei l√§ngeren Texten kam es jedoch vereinzelt zu
					Wiederholungen wie ‚ÄûAm Freitag Am Freitag‚Äú, was auf eine begrenzte
					Kontextverarbeitung oder √úberanpassung hindeutet. Um dieses
					Verhalten zu mildern, wurde eine sogenannte Temperature-Skalierung
					mit einem Wert von 1.0 eingesetzt. Dabei wird die
					Wahrscheinlichkeitsverteilung √ºber m√∂gliche n√§chste W√∂rter vor der
					Auswahl gegl√§ttet, sodass auch weniger wahrscheinliche Alternativen
					mit einbezogen werden.</p>
				<p>Das Modell reagiert empfindlich auf Gro√ü-/Kleinschreibung und
					Zeichensetzung, was die Qualit√§t der Vorhersage beeinflussen kann.</p>
				<p>Die Begrenzung des Vokabulars auf 10.000 W√∂rter erwies sich
					als sinnvoller Kompromiss zwischen Modellgr√∂√üe und Generalisierung.</p>
				<p>Besonders hilfreich war das Top-k-Ranking zur Bewertung der
					Vorhersagequalit√§t. Die Perplexity als Metrik zeigte zudem, ob das
					Modell sinnvoll generalisiert. Insgesamt wurde deutlich, dass schon
					einfache LSTM-Modelle mit wenig Code und guter Vorverarbeitung
					ansprechende Ergebnisse liefern k√∂nnen. Gleichzeitig zeigte sich,
					dass die Auswahl und Aufbereitung der Trainingsdaten einen gro√üen
					Einfluss auf die Modellqualit√§t hat.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der L√∂sung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow / Keras</strong>: Zum Erstellen,
						Trainieren und Speichern des LSTM-basierten Language Models
						(Sequential, LSTM, Embedding, Adam, EarlyStopping, etc.).</li>
					<li><strong>TensorFlow.js</strong>: Zum Konvertieren des
						Keras-Modells in ein browserkompatibles Format f√ºr die
						clientseitige Vorhersage.</li>
					<li><strong>NumPy</strong>: F√ºr die effiziente Verarbeitung
						von Trainingsdaten (z.‚ÄØB. Sequenzen, Labels, Arrays).</li>
					<li><strong>re (Regular Expressions)</strong>: Zum Aufbereiten
						und Tokenisieren des Textkorpus (z.‚ÄØB. Satzzeichen trennen).</li>
					<li><strong>HTML, CSS, JavaScript</strong>: Zum Aufbau der
						Web-Oberfl√§che inklusive Texteingabe, Vorhersageanzeige und
						Interaktion mit dem TensorFlow.js-Modell im Browser.</li>
				</ul>
				<p>Das Modell nutzt ein gestapeltes LSTM mit Dropout und
					recurrent_dropout, um Overfitting zu vermeiden. Die Trainingsdaten
					werden mithilfe eines festen Werts von 5 W√∂rtern verarbeitet. Das
					Modell wird mit EarlyStopping trainiert und in .keras und .tfjs
					exportiert. Im Frontend wird durch Padding und Begrenzung der
					Wortindizes sichergestellt, dass Eingabesequenzen modellkompatibel
					bleiben. Im Testskript wird zus√§tzlich die Top-k-Genauigkeit und
					Perplexity als Evaluationsma√ü berechnet.</p>

				<h3>2) Fachlich</h3>
				<p>
					Die vorliegende Anwendung basiert auf einem autoregressiven Modell
					zur <strong>n√§chsten Wortvorhersage</strong>, das mithilfe eines
					LSTM-Netzwerks realisiert wurde. Grundlage f√ºr das Training bildet
					ein deutschsprachiger Korpus, der aus dem <a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">Wortschatz-Korpus der Universit√§t Leipzig</a>
					stammt. Hieraus wurden 200.000 Zeilen eingelesen. Der Text wurde im
					Vorfeld bereinigt und tokenisiert, wobei insbesondere Satzzeichen
					separiert und Gro√ü-/Kleinschreibung erhalten blieben.
				</p>

				<p>
					Das Modell verarbeitet Wortsequenzen mit einer L√§nge von f√ºnf
					Tokens und sagt jeweils das darauffolgende Wort vorher. Es wurde
					mit zwei gestapelten LSTM-Schichten (je 200 Einheiten) trainiert
					und durch Dropout regularisiert. W√§hrend des Trainings wurde
					<code>EarlyStopping</code>
					verwendet, um √úberanpassung zu vermeiden. Als Optimierer kam
					<code>Adam</code>
					mit einer Lernrate von 0,001 zum Einsatz.
				</p>

				<p>Nach erfolgreichem Training wurde das Modell ins
					TensorFlow.js-Format konvertiert und in eine interaktive
					Weboberfl√§che integriert. Diese erlaubt es Nutzer:innen, eigene
					Texte einzugeben, automatische Wortvorschl√§ge zu generieren oder
					Schritt f√ºr Schritt Texte fortzusetzen. Die Vorschl√§ge basieren auf
					den wahrscheinlichsten n√§chsten W√∂rtern laut Modell ‚Äì inklusive
					Prozentwerten.</p>

				<p>
					Zur qualitativen Bewertung des Modells wurden neben der klassischen
					Genauigkeit auch <em>Top-k-Genauigkeiten</em> (k=1, 5, 10, 20, 100)
					und die <strong>Perplexity</strong> als Kennzahlen berechnet. Diese
					geben Aufschluss √ºber die F√§higkeit des Modells, plausible
					Folgew√∂rter im Sprachkontext zu w√§hlen.
				</p>
				<p>Um den Nutzer Hilfestellungen zu geben, wurden an den
					wichtigsten Stellen Tooltips integriert. F√ºr die Barrierefreiheit
					wurde versucht alles zu ber√ºcksichtigen. Dazu geh√∂ren
					beispielsweise aria-label, alt-texte oder Kontraste bei der
					Farbauswahl.</p>
				<p>
					<strong>Quellen & verwendete Technologien:</strong><br>
					Wortschatz-Korpus der Universit√§t Leipzig<br> (<a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">wortschatz.uni-leipzig.de</a>)<br> TensorFlow
					und Keras ‚Äì f√ºr Modellarchitektur, Training und Export<br> (<a
						href="https://www.tensorflow.org/" target="_blank">tensorflow.org</a>)<br>

					TensorFlow.js ‚Äì f√ºr die Modellnutzung im Browser<br> (<a
						href="https://js.tensorflow.org" target="_blank">js.tensorflow.org</a>)<br>

					NumPy ‚Äì f√ºr Array-Manipulation im Trainingsprozess<br> (<a
						href="https://numpy.org" target="_blank">numpy.org</a>)<br>

					Regul√§re Ausdr√ºcke (re) ‚Äì zur Textvorverarbeitung in Python<br>
					(<a href="https://docs.python.org/3/library/re.html"
						target="_blank">python.org ‚Ä∫ re</a>)<br> Chart.js ‚Äì f√ºr
					Visualisierungen im Frontend (optional/erg√§nzend)<br> (<a
						href="https://www.chartjs.org" target="_blank">chartjs.org</a>)<br>

					HTML, CSS, JavaScript (ES6) ‚Äì f√ºr die Benutzeroberfl√§che,
					Eingabelogik und Eventsteuerung<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web"
						target="_blank">MDN Web Docs</a>)<br> Fetch API ‚Äì zum Laden
					des Tokenizer-Wortindex im Browser<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"
						target="_blank">MDN Fetch API</a>)<br> Google Fonts ‚Äì f√ºr
					typografische Gestaltung<br> (<a
						href="https://fonts.google.com" target="_blank">fonts.google.com</a>)<br>

					Vorlesungsfolien, √úbungen und Diskussionen im Rahmen der
					Lehrveranstaltung
				</p>

			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
