<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">üåô Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt ‚ÄûDaten‚Äú unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angeh√§ngt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen m√∂glich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z.‚ÄØB.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>I1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollst√§ndigen, durch Leerzeichen
						getrennten W√∂rtern bestehen. Durch Klick auf den Button <em>‚ÄûVorhersage‚Äú</em>
						werden die wahrscheinlichsten folgenden W√∂rter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						W√∂rter ausw√§hlen, welches dann angeh√§ngt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>I2)</strong> Mit dem Button <em>‚ÄûWeiter‚Äú</em> kann
						das wahrscheinlichste Wort automatisch angeh√§ngt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>I3)</strong> √úber den Button <em>‚ÄûAuto‚Äú</em>
						k√∂nnen automatisch bis zu 10 W√∂rter vorhergesagt werden. Dieser
						Vorgang kann mit dem Button <em>‚ÄûStopp‚Äú</em> unterbrochen werden.</li>
					<li><strong>I4)</strong> Mit dem Button <em>‚ÄûReset‚Äú</em>
						werden sowohl der eingegebene Text als auch das Netzwerk
						zur√ºckgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Wortauswahl (bei I1)</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begr√ºnden Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte n√§chste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zus√§tzliches Ma√ü.</li>
					<li>Untersuchen Sie, ob sich die urspr√ºnglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						m√∂gliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">
					<button id="predictBtn">üîÆ Vorhersage</button>
					<button id="weiterBtn">‚û°Ô∏è Weiter</button>
					<button id="autoBtn">ü§ñ Auto</button>
					<button id="stopBtn">üõë Stopp</button>
					<button id="resetBtn">üîÑ Reset</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschl√§ge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingef√ºgte Vorschl√§ge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzl√§nge von 5
							Tokens. Dies erm√∂glicht es dem Modell, sprachliche Abh√§ngigkeiten
							√ºber einen gr√∂√üeren Bereich zu erfassen.</p>

						<p>An das Embedding schlie√üen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich l√§nger wurde. Die Erh√∂hung der Units hat
							zu einer Verbesserung des Trainingsergebnisses gef√ºhrt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,3 sowie ein recurrent Dropout von 0,2 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollst√§ndige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), w√§hrend die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repr√§sentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt √ºber eine dichte Softmax-Schicht, deren
							Dimension der Gr√∂√üe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							√ºber alle potenziell n√§chsten W√∂rter zur√ºck.</p>

						<p>F√ºr das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Gr√∂√üe von 32 √ºber 60
							Epochen hinweg trainiert. Die Anzahl der Epochen hat sich als
							 am besten herausgestellt, weil loss und accuracy am dichtesten 
							 beieinander waren.</p>

						<p>Diese Architektur erm√∂glicht eine stabile, regulierte
							Modellierung sprachlicher Abh√§ngigkeiten mit verbesserter
							Generalisierungsf√§higkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>
					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						Teilweise m√∂glich. Das Modell kann l√§ngere Sequenzen korrekt
						erg√§nzen. Dies kann unter Umst√§nden Datenschutzprobleme aufwerfen,
						z.‚ÄØB. wenn vertrauliche Formulierungen reproduziert werden.</li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>F√ºr die Regression der gegebenen Funktion wurde ein
					Feedforward Neural Network (FFNN) mit konfigurierbarer Architektur
					eingesetzt (2 Hidden-Layer, je 100 Neuronen, ReLU-Aktivierung,
					Adam-Optimizer mit Lernrate 0.01 und Batch-Size 32). Die Anzahl der
					Datenpunkte (N) und die Rauschvarianz (V) wurden gem√§√ü Vorgabe
					flexibel gestaltet (hier typischerweise N = 100, V = 0.05). Dadurch
					konnten gezielt Auswirkungen von Datenmenge und Rauschst√§rke auf
					das Modellverhalten untersucht werden.</p>
				<p>
					Auf den <strong>unverrauschten Daten</strong> zeigte das Modell bei
					einer Epochenzahl von 1000 ein sehr gutes Fit-Verhalten (Train- und
					Test-MSE < 0.0001). Bei den <strong>verrauschten Daten</strong>
					f√ºhrte ein Bereich von ca. 100-300 Epochen zu einer ausgewogenen
					Generalisierung (Train-MSE ~0.04-0.06, Test-MSE ~0.06-0.08). Bei
					sehr hoher Epochenanzahl (ab ca. 15.000) konnte Overfitting
					deutlich beobachtet werden: der Train-Loss sinkt weiter (&lt;0.02),
					w√§hrend der Test-Loss ansteigt (&gt;0.12 bei 15.000 und &gt;0.32
					bei 20.000).
				</p>
				<p>√Ñnderungen an der Architektur (z.B. zus√§tzliche Layer oder
					mehr Neuronen) zeigten, dass komplexere Modelle tendenziell
					schneller zu Overfitting neigen, insbesondere bei kleinen
					Datens√§tzen oder hoher Rauschvarianz. Ebenso wirkt sich die Wahl
					der Aktivierungsfunktion auf die Lernstabilit√§t aus (ReLU erwies
					sich hier als robust). Die Erh√∂hung der Datenpunkte f√ºhrte hingegen
					zu einem schnelleren Best-Fit Verhalten und einer Angleichung
					zwischen Test- und Trainings-MSE sowie einem viel sp√§teren
					Overt-Fit.</p>
				<p>Insgesamt hat dieses Experiment sehr anschaulich
					verdeutlicht, wie Modellkomplexit√§t, Trainingsdauer, Datenmenge und
					Rauschlevel gemeinsam das Lern- und Generalisierungsverhalten eines
					neuronalen Netzes bestimmen.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der L√∂sung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow.js</strong>: zur Implementierung und
						Ausf√ºhrung des neuronalen Netzes (FFNN) direkt im Browser
						(Training, Evaluation und Inferenz).</li>
					<li><strong>Chart.js</strong>: zur Visualisierung der
						Lernkurven (Train- und Test-Loss √ºber Epochen).</li>
				</ul>
				<p>
					Technische Besonderheiten der L√∂sung: Das gesamte Training und die
					Modellberechnungen erfolgen vollst√§ndig clientseitig im Browser mit
					TensorFlow.js, ohne Server oder Backend. Die Lernkurve wird
					performant aufgebaut, indem der Train-Loss direkt aus
					<code>fit()</code>
					und der Test-Loss gezielt mit
					<code>evaluate()</code>
					ermittelt wird. Zus√§tzlich sind interaktive Steuerelemente (Slider
					und Dropdowns) integriert, um Datenparameter (N, Rauschvarianz) und
					Modellarchitektur (Layer, Neuronen, Aktivierungsfunktion) dynamisch
					anzupassen. Eine Fortschrittsanzeige informiert w√§hrend des
					Trainings √ºber den aktuellen Stand.
				</p>

				<h3>2) Fachlich</h3>
				<p>Die Logik der Implementierung orientiert sich an der
					Aufgabenstellung: Zun√§chst wird ein unverrauschter Datensatz
					generiert und f√ºr das Modelltraining genutzt. Anschlie√üend werden
					verrauschte Daten erzeugt und in zwei Varianten trainiert (Best-Fit
					und Overfit), um die Auswirkungen von Epochenzahl und
					Modellkomplexit√§t auf die Generalisierungsf√§higkeit zu untersuchen.
					F√ºr das Training wird der Adam-Optimizer mit Lernrate 0.01 und
					Batch-Size 32 eingesetzt. Die Modelle bestehen aus 1‚Äì3
					Hidden-Layern mit konfigurierbarer Neuronenzahl und
					ReLU-Aktivierung; der Output-Layer ist linear.</p>
				<p>Durch die einheitliche und kontrollierte Visualisierung der
					Lernkurven sowie der Modellvorhersagen konnten wichtige Effekte wie
					Overfitting und Bias-Variance-Verhalten beobachtet und
					nachvollzogen werden. Besonderer Wert wurde darauf gelegt, die
					Testdaten strikt nur zur Evaluation zu verwenden, um eine
					realistische Einsch√§tzung der Modellgeneraliserung zu
					gew√§hrleisten.</p>
				<p>Um den Nutzer Hilfestellungen zu geben, wurden an den
					wichtigsten Stellen Tooltips integriert. F√ºr die Barrierefreiheit
					wurde versucht alles zu ber√ºcksichtigen. Dazu geh√∂ren
					beispielsweise aria-label, alt-texte oder Kontraste bei der
					Farbauswahl.</p>
				<p>
					Quellen: TensorFlow.js Dokumentation (<a
						href="https://js.tensorflow.org" target="_blank">https://js.tensorflow.org</a>),
					Chart.js Dokumentation (<a href="https://www.chartjs.org"
						target="_blank">https://www.chartjs.org</a>), begleitende
					Vorlesungsfolien und √úbungsmaterialien.
				</p>
			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
