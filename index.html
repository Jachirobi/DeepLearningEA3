<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<link rel="stylesheet"
	href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">🌙 Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt „Daten“ unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angehängt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen möglich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z. B.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollständigen, durch Leerzeichen
						getrennten Wörtern bestehen. Durch Klick auf den Button <em>„Vorhersage“</em>
						werden die wahrscheinlichsten folgenden Wörter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						Wörter auswählen, welches dann angehängt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>2)</strong> Mit dem Button <em>„Weiter“</em> kann
						das wahrscheinlichste Wort automatisch angehängt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>3)</strong> Über den Button <em>„Auto“</em> können
						automatisch bis zu 10 Wörter vorhergesagt werden.</li>
					<li><strong>4)</strong> Dieser Vorgang kann mit dem Button <em>„Stopp“</em>
						unterbrochen werden.</li>
					<li><strong>5)</strong> Mit dem Button <em>„Reset“</em> werden
						sowohl der eingegebene Text als auch das Netzwerk zurückgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Reset</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begründen Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte nächste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zusätzliches Maß.</li>
					<li>Untersuchen Sie, ob sich die ursprünglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						mögliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">

					<button id="predictBtn">
						<i class="fas fa-magic"></i> Vorhersage
					</button>
					<button id="weiterBtn">
						<i class="fas fa-forward"></i> Weiter
					</button>
					<button id="autoBtn">
						<i class="fas fa-robot"></i> Auto
					</button>
					<button id="stopBtn">
						<i class="fas fa-hand-paper"></i> Stopp
					</button>
					<button id="resetBtn">
						<i class="fas fa-redo"></i> Reset
					</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschläge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingefügte Vorschläge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzlänge von 5
							Token. Dies ermöglicht es dem Modell, sprachliche Abhängigkeiten
							über einen größeren Bereich zu erfassen.</p>

						<p>An das Embedding schließen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich länger wurde. Die Erhöhung der Units hat
							zu einer Verbesserung des Trainingsergebnisses geführt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,2 sowie ein recurrent Dropout von 0,1 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollständige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), während die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repräsentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt über eine dichte Softmax-Schicht, deren
							Dimension der Größe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							über alle potenziell nächsten Wörter zurück.</p>

						<p>Für das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Größe von 128 über 9
							Epochen hinweg trainiert. Die Anzahl der Epochen hat wurde
							mittels early stoppings ermittelt.</p>

						<p>Diese Architektur ermöglicht eine stabile, regulierte
							Modellierung sprachlicher Abhängigkeiten mit verbesserter
							Generalisierungsfähigkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>

					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						<p>Das trainierte LSTM-Sprachmodell speichert keine konkreten
							Trainingssätze, sondern lediglich statistische Muster in seinen
							Gewichtungen. Eine direkte Rekonstruktion der ursprünglichen
							Trainingsdaten ist daher nicht möglich.</p>

						<p>Allerdings kann es bei häufigen oder prägnanten
							Formulierungen, insbesondere bei kleinem Datensatz oder
							Overfitting, vorkommen, dass Teile des Trainingsmaterials
							sinngemäß oder sogar wörtlich reproduziert werden. In solchen
							Fällen besteht ein gewisses Datenschutzrisiko, insbesondere wenn
							personenbezogene Daten im Training enthalten waren. Auch wenn das
							Risiko bei einem Modell mit begrenztem Vokabular (z. B. 5000
							Wörter) und normalisierten Daten gering ist, sollte man bei der
							Veröffentlichung des Modells auf mögliche Rückschlüsse auf
							sensible Inhalte achten.</p></li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Das trainierte LSTM-Modell zeigte eine grundsätzlich
					funktionierende Vorhersagefähigkeit für häufige Wortfolgen,
					insbesondere bei klaren syntaktischen Mustern.</p>
				<p>Bei längeren Texten kam es jedoch vereinzelt zu
					Wiederholungen wie „Am Freitag Am Freitag“, was auf eine begrenzte
					Kontextverarbeitung oder Überanpassung hindeutet. Um dieses
					Verhalten zu mildern, wurde eine sogenannte Temperature-Skalierung
					mit einem Wert von 1.0 eingesetzt. Dabei wird die
					Wahrscheinlichkeitsverteilung über mögliche nächste Wörter vor der
					Auswahl geglättet, sodass auch weniger wahrscheinliche Alternativen
					mit einbezogen werden.</p>
				<p>Das Modell reagiert empfindlich auf Groß-/Kleinschreibung und
					Zeichensetzung, was die Qualität der Vorhersage beeinflussen kann.</p>
				<p>Die Begrenzung des Vokabulars auf 10.000 Wörter erwies sich
					als sinnvoller Kompromiss zwischen Modellgröße und Generalisierung.</p>
				<p>Besonders hilfreich war das Top-k-Ranking zur Bewertung der
					Vorhersagequalität. Die Perplexity als Metrik zeigte zudem, ob das
					Modell sinnvoll generalisiert. Insgesamt wurde deutlich, dass schon
					einfache LSTM-Modelle mit wenig Code und guter Vorverarbeitung
					ansprechende Ergebnisse liefern können. Gleichzeitig zeigte sich,
					dass die Auswahl und Aufbereitung der Trainingsdaten einen großen
					Einfluss auf die Modellqualität hat.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der Lösung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow / Keras</strong>: Zum Erstellen,
						Trainieren und Speichern des LSTM-basierten Language Models
						(Sequential, LSTM, Embedding, Adam, EarlyStopping, etc.).</li>
					<li><strong>TensorFlow.js</strong>: Zum Konvertieren des
						Keras-Modells in ein browserkompatibles Format für die
						clientseitige Vorhersage.</li>
					<li><strong>NumPy</strong>: Für die effiziente Verarbeitung
						von Trainingsdaten (z. B. Sequenzen, Labels, Arrays).</li>
					<li><strong>re (Regular Expressions)</strong>: Zum Aufbereiten
						und Tokenisieren des Textkorpus (z. B. Satzzeichen trennen).</li>
					<li><strong>HTML, CSS, JavaScript</strong>: Zum Aufbau der
						Web-Oberfläche inklusive Texteingabe, Vorhersageanzeige und
						Interaktion mit dem TensorFlow.js-Modell im Browser.</li>
				</ul>
				<p>Das Modell nutzt ein gestapeltes LSTM mit Dropout und
					recurrent_dropout, um Overfitting zu vermeiden. Die Trainingsdaten
					werden mithilfe eines festen Werts von 5 Wörtern verarbeitet. Das
					Modell wird mit EarlyStopping trainiert und in .keras und .tfjs
					exportiert. Im Frontend wird durch Padding und Begrenzung der
					Wortindizes sichergestellt, dass Eingabesequenzen modellkompatibel
					bleiben. Im Testskript wird zusätzlich die Top-k-Genauigkeit und
					Perplexity als Evaluationsmaß berechnet.</p>

				<h3>2) Fachlich</h3>
				<p>
					Die vorliegende Anwendung basiert auf einem autoregressiven Modell
					zur <strong>nächsten Wortvorhersage</strong>, das mithilfe eines
					LSTM-Netzwerks realisiert wurde. Grundlage für das Training bildet
					ein deutschsprachiger Korpus, der aus dem <a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">Wortschatz-Korpus der Universität Leipzig</a>
					stammt. Hieraus wurden 200.000 Zeilen eingelesen. Der Text wurde im
					Vorfeld bereinigt und tokenisiert, wobei insbesondere Satzzeichen
					separiert und Groß-/Kleinschreibung erhalten blieben.
				</p>

				<p>
					Das Modell verarbeitet Wortsequenzen mit einer Länge von fünf
					Tokens und sagt jeweils das darauffolgende Wort vorher. Es wurde
					mit zwei gestapelten LSTM-Schichten (je 200 Einheiten) trainiert
					und durch Dropout regularisiert. Während des Trainings wurde
					<code>EarlyStopping</code>
					verwendet, um Überanpassung zu vermeiden. Als Optimierer kam
					<code>Adam</code>
					mit einer Lernrate von 0,001 zum Einsatz.
				</p>

				<p>Nach erfolgreichem Training wurde das Modell ins
					TensorFlow.js-Format konvertiert und in eine interaktive
					Weboberfläche integriert. Diese erlaubt es Nutzer:innen, eigene
					Texte einzugeben, automatische Wortvorschläge zu generieren oder
					Schritt für Schritt Texte fortzusetzen. Die Vorschläge basieren auf
					den wahrscheinlichsten nächsten Wörtern laut Modell – inklusive
					Prozentwerten.</p>

				<p>
					Zur qualitativen Bewertung des Modells wurden neben der klassischen
					Genauigkeit auch <em>Top-k-Genauigkeiten</em> (k=1, 5, 10, 20, 100)
					und die <strong>Perplexity</strong> als Kennzahlen berechnet. Diese
					geben Aufschluss über die Fähigkeit des Modells, plausible
					Folgewörter im Sprachkontext zu wählen.
				</p>
				<p>Um den Nutzer Hilfestellungen zu geben, wurden an den
					wichtigsten Stellen Tooltips integriert. Für die Barrierefreiheit
					wurde versucht alles zu berücksichtigen. Dazu gehören
					beispielsweise aria-label, alt-texte oder Kontraste bei der
					Farbauswahl.</p>
				<p>
					<strong>Quellen & verwendete Technologien:</strong><br>
					Wortschatz-Korpus der Universität Leipzig<br> (<a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">wortschatz.uni-leipzig.de</a>)<br> TensorFlow
					und Keras – für Modellarchitektur, Training und Export<br> (<a
						href="https://www.tensorflow.org/" target="_blank">tensorflow.org</a>)<br>

					TensorFlow.js – für die Modellnutzung im Browser<br> (<a
						href="https://js.tensorflow.org" target="_blank">js.tensorflow.org</a>)<br>

					NumPy – für Array-Manipulation im Trainingsprozess<br> (<a
						href="https://numpy.org" target="_blank">numpy.org</a>)<br>

					Reguläre Ausdrücke (re) – zur Textvorverarbeitung in Python<br>
					(<a href="https://docs.python.org/3/library/re.html"
						target="_blank">python.org › re</a>)<br> Chart.js – für
					Visualisierungen im Frontend (optional/ergänzend)<br> (<a
						href="https://www.chartjs.org" target="_blank">chartjs.org</a>)<br>

					HTML, CSS, JavaScript (ES6) – für die Benutzeroberfläche,
					Eingabelogik und Eventsteuerung<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web"
						target="_blank">MDN Web Docs</a>)<br> Fetch API – zum Laden
					des Tokenizer-Wortindex im Browser<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"
						target="_blank">MDN Fetch API</a>)<br> Google Fonts – für
					typografische Gestaltung<br> (<a
						href="https://fonts.google.com" target="_blank">fonts.google.com</a>)<br>

					Vorlesungsfolien, Übungen und Diskussionen im Rahmen der
					Lehrveranstaltung
				</p>

			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
