<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">🌙 Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt „Daten“ unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angehängt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen möglich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z. B.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>I1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollständigen, durch Leerzeichen
						getrennten Wörtern bestehen. Durch Klick auf den Button <em>„Vorhersage“</em>
						werden die wahrscheinlichsten folgenden Wörter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						Wörter auswählen, welches dann angehängt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>I2)</strong> Mit dem Button <em>„Weiter“</em> kann
						das wahrscheinlichste Wort automatisch angehängt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>I3)</strong> Über den Button <em>„Auto“</em>
						können automatisch bis zu 10 Wörter vorhergesagt werden. Dieser
						Vorgang kann mit dem Button <em>„Stopp“</em> unterbrochen werden.</li>
					<li><strong>I4)</strong> Mit dem Button <em>„Reset“</em>
						werden sowohl der eingegebene Text als auch das Netzwerk
						zurückgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Wortauswahl (bei I1)</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begründen Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte nächste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zusätzliches Maß.</li>
					<li>Untersuchen Sie, ob sich die ursprünglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						mögliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">
					<button id="predictBtn">🔮 Vorhersage</button>
					<button id="weiterBtn">➡️ Weiter</button>
					<button id="autoBtn">🤖 Auto</button>
					<button id="stopBtn">🛑 Stopp</button>
					<button id="resetBtn">🔄 Reset</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschläge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingefügte Vorschläge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzlänge von 5
							Tokens. Dies ermöglicht es dem Modell, sprachliche Abhängigkeiten
							über einen größeren Bereich zu erfassen.</p>

						<p>An das Embedding schließen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich länger wurde. Die Erhöhung der Units hat
							zu einer Verbesserung des Trainingsergebnisses geführt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,3 sowie ein recurrent Dropout von 0,2 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollständige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), während die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repräsentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt über eine dichte Softmax-Schicht, deren
							Dimension der Größe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							über alle potenziell nächsten Wörter zurück.</p>

						<p>Für das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Größe von 32 über 60
							Epochen hinweg trainiert. Die Anzahl der Epochen hat sich als
							 am besten herausgestellt, weil loss und accuracy am dichtesten 
							 beieinander waren.</p>

						<p>Diese Architektur ermöglicht eine stabile, regulierte
							Modellierung sprachlicher Abhängigkeiten mit verbesserter
							Generalisierungsfähigkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>
					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						Teilweise möglich. Das Modell kann längere Sequenzen korrekt
						ergänzen. Dies kann unter Umständen Datenschutzprobleme aufwerfen,
						z. B. wenn vertrauliche Formulierungen reproduziert werden.</li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Für die Regression der gegebenen Funktion wurde ein
					Feedforward Neural Network (FFNN) mit konfigurierbarer Architektur
					eingesetzt (2 Hidden-Layer, je 100 Neuronen, ReLU-Aktivierung,
					Adam-Optimizer mit Lernrate 0.01 und Batch-Size 32). Die Anzahl der
					Datenpunkte (N) und die Rauschvarianz (V) wurden gemäß Vorgabe
					flexibel gestaltet (hier typischerweise N = 100, V = 0.05). Dadurch
					konnten gezielt Auswirkungen von Datenmenge und Rauschstärke auf
					das Modellverhalten untersucht werden.</p>
				<p>
					Auf den <strong>unverrauschten Daten</strong> zeigte das Modell bei
					einer Epochenzahl von 1000 ein sehr gutes Fit-Verhalten (Train- und
					Test-MSE < 0.0001). Bei den <strong>verrauschten Daten</strong>
					führte ein Bereich von ca. 100-300 Epochen zu einer ausgewogenen
					Generalisierung (Train-MSE ~0.04-0.06, Test-MSE ~0.06-0.08). Bei
					sehr hoher Epochenanzahl (ab ca. 15.000) konnte Overfitting
					deutlich beobachtet werden: der Train-Loss sinkt weiter (&lt;0.02),
					während der Test-Loss ansteigt (&gt;0.12 bei 15.000 und &gt;0.32
					bei 20.000).
				</p>
				<p>Änderungen an der Architektur (z.B. zusätzliche Layer oder
					mehr Neuronen) zeigten, dass komplexere Modelle tendenziell
					schneller zu Overfitting neigen, insbesondere bei kleinen
					Datensätzen oder hoher Rauschvarianz. Ebenso wirkt sich die Wahl
					der Aktivierungsfunktion auf die Lernstabilität aus (ReLU erwies
					sich hier als robust). Die Erhöhung der Datenpunkte führte hingegen
					zu einem schnelleren Best-Fit Verhalten und einer Angleichung
					zwischen Test- und Trainings-MSE sowie einem viel späteren
					Overt-Fit.</p>
				<p>Insgesamt hat dieses Experiment sehr anschaulich
					verdeutlicht, wie Modellkomplexität, Trainingsdauer, Datenmenge und
					Rauschlevel gemeinsam das Lern- und Generalisierungsverhalten eines
					neuronalen Netzes bestimmen.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der Lösung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow.js</strong>: zur Implementierung und
						Ausführung des neuronalen Netzes (FFNN) direkt im Browser
						(Training, Evaluation und Inferenz).</li>
					<li><strong>Chart.js</strong>: zur Visualisierung der
						Lernkurven (Train- und Test-Loss über Epochen).</li>
				</ul>
				<p>
					Technische Besonderheiten der Lösung: Das gesamte Training und die
					Modellberechnungen erfolgen vollständig clientseitig im Browser mit
					TensorFlow.js, ohne Server oder Backend. Die Lernkurve wird
					performant aufgebaut, indem der Train-Loss direkt aus
					<code>fit()</code>
					und der Test-Loss gezielt mit
					<code>evaluate()</code>
					ermittelt wird. Zusätzlich sind interaktive Steuerelemente (Slider
					und Dropdowns) integriert, um Datenparameter (N, Rauschvarianz) und
					Modellarchitektur (Layer, Neuronen, Aktivierungsfunktion) dynamisch
					anzupassen. Eine Fortschrittsanzeige informiert während des
					Trainings über den aktuellen Stand.
				</p>

				<h3>2) Fachlich</h3>
				<p>Die Logik der Implementierung orientiert sich an der
					Aufgabenstellung: Zunächst wird ein unverrauschter Datensatz
					generiert und für das Modelltraining genutzt. Anschließend werden
					verrauschte Daten erzeugt und in zwei Varianten trainiert (Best-Fit
					und Overfit), um die Auswirkungen von Epochenzahl und
					Modellkomplexität auf die Generalisierungsfähigkeit zu untersuchen.
					Für das Training wird der Adam-Optimizer mit Lernrate 0.01 und
					Batch-Size 32 eingesetzt. Die Modelle bestehen aus 1–3
					Hidden-Layern mit konfigurierbarer Neuronenzahl und
					ReLU-Aktivierung; der Output-Layer ist linear.</p>
				<p>Durch die einheitliche und kontrollierte Visualisierung der
					Lernkurven sowie der Modellvorhersagen konnten wichtige Effekte wie
					Overfitting und Bias-Variance-Verhalten beobachtet und
					nachvollzogen werden. Besonderer Wert wurde darauf gelegt, die
					Testdaten strikt nur zur Evaluation zu verwenden, um eine
					realistische Einschätzung der Modellgeneraliserung zu
					gewährleisten.</p>
				<p>Um den Nutzer Hilfestellungen zu geben, wurden an den
					wichtigsten Stellen Tooltips integriert. Für die Barrierefreiheit
					wurde versucht alles zu berücksichtigen. Dazu gehören
					beispielsweise aria-label, alt-texte oder Kontraste bei der
					Farbauswahl.</p>
				<p>
					Quellen: TensorFlow.js Dokumentation (<a
						href="https://js.tensorflow.org" target="_blank">https://js.tensorflow.org</a>),
					Chart.js Dokumentation (<a href="https://www.chartjs.org"
						target="_blank">https://www.chartjs.org</a>), begleitende
					Vorlesungsfolien und Übungsmaterialien.
				</p>
			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
