<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<link rel="stylesheet"
	href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link
	href="https://cdn.jsdelivr.net/npm/prismjs@1/themes/prism-tomorrow.css"
	rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs@1/prism.min.js"></script>
<script
	src="https://cdn.jsdelivr.net/npm/prismjs@1/components/prism-python.min.js"></script>
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">üåô Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt ‚ÄûDaten‚Äú unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angeh√§ngt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen m√∂glich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z.‚ÄØB.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollst√§ndigen, durch Leerzeichen
						getrennten W√∂rtern bestehen. Durch Klick auf den Button <em>‚ÄûVorhersage‚Äú</em>
						werden die wahrscheinlichsten folgenden W√∂rter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						W√∂rter ausw√§hlen, welches dann angeh√§ngt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>2)</strong> Mit dem Button <em>‚ÄûWeiter‚Äú</em> kann
						das wahrscheinlichste Wort automatisch angeh√§ngt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>3)</strong> √úber den Button <em>‚ÄûAuto‚Äú</em> k√∂nnen
						automatisch bis zu 10 W√∂rter vorhergesagt werden.</li>
					<li><strong>4)</strong> Dieser Vorgang kann mit dem Button <em>‚ÄûStopp‚Äú</em>
						unterbrochen werden.</li>
					<li><strong>5)</strong> Mit dem Button <em>‚ÄûReset‚Äú</em> werden
						sowohl der eingegebene Text als auch das Netzwerk zur√ºckgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Reset</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begr√ºnden Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte n√§chste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zus√§tzliches Ma√ü.</li>
					<li>Untersuchen Sie, ob sich die urspr√ºnglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						m√∂gliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext (optimal 5 Worte zur Wortvorhersage):</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">

					<button id="predictBtn">
						<i class="fas fa-magic"></i> Vorhersage
					</button>
					<button id="weiterBtn">
						<i class="fas fa-forward"></i> Weiter
					</button>
					<button id="autoBtn">
						<i class="fas fa-robot"></i> Auto
					</button>
					<button id="stopBtn">
						<i class="fas fa-hand-paper"></i> Stopp
					</button>
					<button id="resetBtn">
						<i class="fas fa-redo"></i> Reset
					</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschl√§ge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingef√ºgte Vorschl√§ge -->
				</ul>
			</div>

		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell beginnt mit einem Embedding-Layer, der jedes
							Token in einen 64-dimensionalen Vektor √ºbersetzt. Die
							Eingabesequenzl√§nge betr√§gt 5 Token, wodurch das Modell
							sprachliche Muster √ºber mehrere W√∂rter hinweg erfassen kann.</p>

						<p>An das Embedding schlie√üen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich l√§nger wurde. Die Erh√∂hung der Units hat
							zu einer Verbesserung des Trainingsergebnisses gef√ºhrt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,2 sowie ein recurrent Dropout von 0,1 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollst√§ndige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), w√§hrend die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repr√§sentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt √ºber eine dichte Softmax-Schicht, deren
							Dimension der Gr√∂√üe des verwendeten Vokabulars entspricht (20.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							√ºber alle potenziell n√§chsten W√∂rter zur√ºck. Als Quelle f√ºr das
							Vokabular wurden 300.000 Zeilen Text eingelesen.</p>

						<p>F√ºr das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Gr√∂√üe von 128 √ºber 9
							Epochen hinweg trainiert. Die Anzahl der Epochen hat wurde
							mittels early stoppings ermittelt.</p>
						<p>F√ºr das Training des Modells wurde ein Python Skript
							verwendet, damit das Modell gespeichert und durch die
							Webanwendung geladen werden kann.</p>

						<p>Das Trainingsskript kann hier eingesehen werden:</p> <details>
							<summary>üìÑ Python-Skript anzeigen (Training_Modell.py)</summary>
							<pre>
								<code class="language-python">
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from keras.layers import Bidirectional
from keras.callbacks import EarlyStopping
from keras.layers import Input
from keras.layers import InputLayer
import numpy as np
import json
import os
import re
import tensorflowjs as tfjs  # tfjs-Konverter direkt im Skript
import tensorflow as tf
print("GPUs verf√ºgbar:", tf.config.list_physical_devices('GPU'))



# Parameter
SEQ_LENGTH = 5
MAX_WORDS = 20000
EPOCHS = 9
BATCH_SIZE = 128

# Funktion: Satzzeichen trennen (wichtig f√ºr Tokenisierung)
def separate_punctuation(text):
    return re.sub(r"([.,!?;:])", r" \1 ", text)

# Textdatei einlesen
with open("deu_news_2024_1M-sentences.txt", "r", encoding="utf-8") as f:
    raw = f.read()

lines = raw.split('\n')[:300000]
sentences = [line.strip().split(maxsplit=1)[-1] for line in lines if line.strip()]
text = " ".join(sentences)
text = separate_punctuation(text).lower()  # Kleinschreibung aktivieren

# Tokenisierung mit Gro√ü-/Kleinschreibung
tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True, oov_token="<UNK>")
tokenizer.fit_on_texts([text])
word_index = tokenizer.word_index
sequences = [token for token in tokenizer.texts_to_sequences([text])[0] if token < MAX_WORDS]

# Eingabesequenzen vorbereiten
inputs, labels = [], []
for i in range(len(sequences) - SEQ_LENGTH):
    inputs.append(sequences[i:i + SEQ_LENGTH])
    labels.append(sequences[i + SEQ_LENGTH])

inputs = np.array(inputs)
labels = np.array(labels)

model_inputs = tf.keras.Input(shape=(SEQ_LENGTH,))
x = Embedding(MAX_WORDS, 256)(model_inputs)
x = LSTM(200, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)(x)
x = LSTM(200, dropout=0.2, recurrent_dropout=0.1)(x)
outputs = Dense(MAX_WORDS, activation='softmax')(x)

model = tf.keras.Model(inputs=model_inputs, outputs=outputs)




model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Training
early_stop = EarlyStopping(
    monitor='val_loss',       # √ºberwacht den Validierungs-Loss
    patience=5,               # stoppt, wenn sich val_loss 5 Epochen lang nicht verbessert
    restore_best_weights=True, # stellt die besten Gewichte wieder her
    verbose=1    
)

model.summary()
model.fit(
    inputs,
    labels,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.1,   # 10‚ÄØ% der Daten werden zur Validierung genutzt
    callbacks=[early_stop],
    verbose=1  # Fortschritt anzeigen
)

# Verzeichnisse anlegen
MODEL_DIR = "saved_model"
TFJS_DIR = "tfjs_model"
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(TFJS_DIR, exist_ok=True)

# Speichern im SavedModel-Format (f√ºr Keras 3 empfohlen)
model.save(MODEL_DIR)
model.save("model.keras")


# Konvertieren nach TensorFlow.js
tfjs.converters.save_keras_model(model, TFJS_DIR)

# Tokenizer-Wortindex speichern
# Begrenze gespeichertes Vokabular auf MAX_WORDS
limited_word_index = {
    word: idx for word, idx in tokenizer.word_index.items() if idx < MAX_WORDS
}
with open(os.path.join(TFJS_DIR, "tokenizer_word_index.json"), "w", encoding="utf-8") as f:
    json.dump(limited_word_index, f)

print("‚úÖ Modell wurde erfolgreich im SavedModel-Format und f√ºr TensorFlow.js gespeichert.")

  
								
								
								
								</code>
							</pre>
						</details>


						<p>Diese Architektur erm√∂glicht eine stabile, regulierte
							Modellierung sprachlicher Abh√§ngigkeiten mit verbesserter
							Generalisierungsf√§higkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>

						<p>Zur Berechnung der Top-k-Vorhersagen und der Perplexity
							wurde das folgende Python Skript verwendet:</p> <details>
							<summary>üìÑ Python-Skript anzeigen (Test_Modell.py)</summary>
							<pre>
								<code class="language-python">
								import numpy as np
import json
import math
import re
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf
from tensorflow.keras.models import load_model

# ------------------------------
# ‚öôÔ∏è Konfiguration
# ------------------------------
SEQ_LENGTH = 5
MAX_WORDS = 20000
BATCH_SIZE = 512

MODEL_PATH = "model.keras"
TOKENIZER_PATH = "tfjs_model/tokenizer_word_index.json"
TESTDATA_PATH = "deu_news_2024_1M-sentences.txt"

# ------------------------------
# üîß Hilfsfunktionen
# ------------------------------
def separate_punctuation(text):
    return re.sub(r"([.,!?;:])", r" \1 ", text)

def load_tokenizer(path):
    with open(path, "r", encoding="utf-8") as f:
        word_index = json.load(f)
    tokenizer = Tokenizer(num_words=MAX_WORDS, lower=False, oov_token="<UNK>")
    tokenizer.word_index = word_index
    tokenizer.index_word = {v: k for k, v in word_index.items()}
    return tokenizer

def evaluate_topk_accuracy(model, inputs, labels, topk_list=[1, 5, 10, 20, 100]):
    results = {k: 0 for k in topk_list}
    total = 0

    for i in range(0, len(inputs), BATCH_SIZE):
        batch_inputs = inputs[i:i + BATCH_SIZE]
        batch_labels = labels[i:i + BATCH_SIZE]
        preds = model.predict(batch_inputs, verbose=0)

        for j, pred in enumerate(preds):
            sorted_indices = np.argsort(pred)[::-1]
            true_index = batch_labels[j]
            for k in topk_list:
                if true_index in sorted_indices[:k]:
                    results[k] += 1
        total += len(batch_inputs)

    return {k: round(results[k] / total * 100, 2) for k in topk_list}

def calculate_perplexity(model, inputs, labels):
    log_probs = []
    for i in range(0, len(inputs), BATCH_SIZE):
        batch_inputs = inputs[i:i + BATCH_SIZE]
        batch_labels = labels[i:i + BATCH_SIZE]
        preds = model.predict(batch_inputs, verbose=0)

        for j, pred in enumerate(preds):
            true_idx = batch_labels[j]
            prob = pred[true_idx] if true_idx < len(pred) else 1e-10
            log_probs.append(math.log(prob + 1e-10))

    cross_entropy = -np.mean(log_probs)
    return round(math.exp(cross_entropy), 2)

# ------------------------------
# Ausf√ºhrung
# ------------------------------
print("Lade Modell...")
model = load_model(MODEL_PATH, compile=False)

print("Lade Tokenizer...")
tokenizer = load_tokenizer(TOKENIZER_PATH)

print(""Lade Testdaten...")
with open(TESTDATA_PATH, "r", encoding="utf-8") as f:
    raw_text = f.read()

lines = raw_text.split('\n')[:10000]
sentences = [line.strip().split(maxsplit=1)[-1] for line in lines if line.strip()]
text = " ".join(sentences)
text = separate_punctuation(text)  # Nur verwenden, wenn Satzzeichen im Tokenizer enthalten sind

sequence = tokenizer.texts_to_sequences([text])[0]

inputs, labels = [], []
for i in range(len(sequence) - SEQ_LENGTH):
    inputs.append(sequence[i:i + SEQ_LENGTH])
    labels.append(sequence[i + SEQ_LENGTH])

inputs = np.array(inputs)
labels = np.array(labels)

print(f"Berechne Top-k-Genauigkeit f√ºr {len(inputs)} Sequenzen...")
topk = evaluate_topk_accuracy(model, inputs, labels)
print("Top-k Accuracy:", topk)

print("Berechne Perplexity...")
perplexity = calculate_perplexity(model, inputs, labels)
print("Perplexity:", perplexity)

# Ergebnis speichern
result = {
    "topk_accuracy": topk,
    "perplexity": perplexity
}

with open("evaluation_result.json", "w", encoding="utf-8") as f:
    json.dump(result, f, indent=2, ensure_ascii=False)

print("Fertig.")
print(json.dumps(result, indent=2, ensure_ascii=False))
								
								
								
								</code>
							</pre>
						</details>
						<p>Die folgende Tabelle zeigt, wie oft das richtige Wort unter
							den Top-k-Vorhersagen lag:</p>
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul>
						<p>
							Perplexity = <span id="perplexity">...</span>
						</p>
						<div id="plotly-chart"
							style="width: 100%; max-width: 700px; height: 400px; margin-top: 1em;"></div>
					</li>

					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						<p>Das trainierte LSTM-Sprachmodell speichert keine konkreten
							Trainingss√§tze, sondern lediglich statistische Muster in seinen
							Gewichtungen. Eine direkte Rekonstruktion der urspr√ºnglichen
							Trainingsdaten ist daher nicht m√∂glich.</p>

						<p>Allerdings kann es bei h√§ufigen oder pr√§gnanten
							Formulierungen, insbesondere bei kleinem Datensatz oder
							Overfitting, vorkommen, dass Teile des Trainingsmaterials
							sinngem√§√ü oder sogar w√∂rtlich reproduziert werden. In solchen
							F√§llen besteht ein gewisses Datenschutzrisiko, insbesondere wenn
							personenbezogene Daten im Training enthalten waren. Auch wenn das
							Risiko bei einem Modell mit begrenztem Vokabular (z.‚ÄØB. 5000
							W√∂rter) und normalisierten Daten gering ist, sollte man bei der
							Ver√∂ffentlichung des Modells auf m√∂gliche R√ºckschl√ºsse auf
							sensible Inhalte achten.</p></li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Das trainierte LSTM-Modell zeigte eine grunds√§tzlich
					funktionierende Vorhersagef√§higkeit f√ºr h√§ufige Wortfolgen,
					insbesondere bei klaren syntaktischen Mustern.</p>
				<p>Bei l√§ngeren Texten kam es jedoch vereinzelt zu
					Wiederholungen wie ‚ÄûAm Freitag Am Freitag‚Äú, was auf eine begrenzte
					Kontextverarbeitung oder √úberanpassung hindeutet. Um dieses
					Verhalten zu mildern, wurde eine sogenannte Temperature-Skalierung
					mit einem Wert von 1.0 eingesetzt. Dabei wird die
					Wahrscheinlichkeitsverteilung √ºber m√∂gliche n√§chste W√∂rter vor der
					Auswahl gegl√§ttet, sodass auch weniger wahrscheinliche Alternativen
					mit einbezogen werden.</p>
				<p>Das Modell reagiert empfindlich auf Gro√ü-/Kleinschreibung und
					Zeichensetzung, was die Qualit√§t der Vorhersage beeinflussen kann.</p>
				<p>Die Begrenzung des Vokabulars auf 20.000 W√∂rter erwies sich
					als sinnvoller Kompromiss zwischen Modellgr√∂√üe und Generalisierung.</p>
				<p>Besonders hilfreich war das Top-k-Ranking zur Bewertung der
					Vorhersagequalit√§t. Die Perplexity als Metrik zeigte zudem, ob das
					Modell sinnvoll generalisiert. Insgesamt wurde deutlich, dass schon
					einfache LSTM-Modelle mit wenig Code und guter Vorverarbeitung
					ansprechende Ergebnisse liefern k√∂nnen. Gleichzeitig zeigte sich,
					dass die Auswahl und Aufbereitung der Trainingsdaten einen gro√üen
					Einfluss auf die Modellqualit√§t hat.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">‚¨áÔ∏è Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der L√∂sung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow / Keras</strong>: Zum Erstellen,
						Trainieren und Speichern des LSTM-basierten Language Models
						(Sequential, LSTM, Embedding, Adam, EarlyStopping, etc.).</li>
					<li><strong>TensorFlow.js</strong>: Zum Konvertieren des
						Keras-Modells in ein browserkompatibles Format f√ºr die
						clientseitige Vorhersage.</li>
					<li><strong>NumPy</strong>: F√ºr die effiziente Verarbeitung
						von Trainingsdaten (z.‚ÄØB. Sequenzen, Labels, Arrays).</li>
					<li><strong>re (Regular Expressions)</strong>: Zum Aufbereiten
						und Tokenisieren des Textkorpus (z.‚ÄØB. Satzzeichen trennen).</li>
					<li><strong>HTML, CSS, JavaScript</strong>: Zum Aufbau der
						Web-Oberfl√§che inklusive Texteingabe, Vorhersageanzeige und
						Interaktion mit dem TensorFlow.js-Modell im Browser.</li>
				</ul>
				<p>Das Modell nutzt ein gestapeltes LSTM mit Dropout und
					recurrent_dropout, um Overfitting zu vermeiden. Die Trainingsdaten
					werden mithilfe eines festen Werts von 5 W√∂rtern verarbeitet. Das
					Modell wird mit EarlyStopping trainiert und in .keras und .tfjs
					exportiert. Im Frontend wird durch Padding und Begrenzung der
					Wortindizes sichergestellt, dass Eingabesequenzen modellkompatibel
					bleiben. Im Testskript wird zus√§tzlich die Top-k-Genauigkeit und
					Perplexity als Evaluationsma√ü berechnet.</p>

				<h3>2) Fachlich</h3>
				<p>
					Die vorliegende Anwendung basiert auf einem autoregressiven Modell
					zur <strong>n√§chsten Wortvorhersage</strong>, das mithilfe eines
					LSTM-Netzwerks realisiert wurde. Grundlage f√ºr das Training bildet
					ein deutschsprachiger Korpus, der aus dem <a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">Wortschatz-Korpus der Universit√§t Leipzig</a>
					stammt. Hieraus wurden 300.000 Zeilen eingelesen. Der Text wurde im
					Vorfeld bereinigt und tokenisiert, wobei insbesondere Satzzeichen
					separiert und Gro√ü-/Kleinschreibung ignoriert wurde.
				</p>

				<p>
					Das Modell verarbeitet Wortsequenzen mit einer L√§nge von f√ºnf
					Tokens und sagt jeweils das darauffolgende Wort vorher. Es wurde
					mit zwei gestapelten LSTM-Schichten (je 200 Einheiten) trainiert
					und durch Dropout regularisiert. W√§hrend des Trainings wurde
					<code>EarlyStopping</code>
					verwendet, um √úberanpassung zu vermeiden. Als Optimierer kam
					<code>Adam</code>
					mit einer Lernrate von 0,001 zum Einsatz.
				</p>

				<p>Nach erfolgreichem Training wurde das Modell ins
					TensorFlow.js-Format konvertiert und in eine interaktive
					Weboberfl√§che integriert. Diese erlaubt es Nutzer:innen, eigene
					Texte einzugeben, automatische Wortvorschl√§ge zu generieren oder
					Schritt f√ºr Schritt Texte fortzusetzen. Die Vorschl√§ge basieren auf
					den wahrscheinlichsten n√§chsten W√∂rtern laut Modell ‚Äì inklusive
					Prozentwerten.</p>

				<p>
					Zur qualitativen Bewertung des Modells wurden neben der klassischen
					Genauigkeit auch <em>Top-k-Genauigkeiten</em> (k=1, 5, 10, 20, 100)
					und die <strong>Perplexity</strong> als Kennzahlen berechnet. Diese
					geben Aufschluss √ºber die F√§higkeit des Modells, plausible
					Folgew√∂rter im Sprachkontext zu w√§hlen.
				</p>
				<p>Um den Nutzer Hilfestellungen zu geben, wurden an den
					wichtigsten Stellen Tooltips integriert. F√ºr die Barrierefreiheit
					wurde versucht alles zu ber√ºcksichtigen. Dazu geh√∂ren
					beispielsweise aria-label, alt-texte oder Kontraste bei der
					Farbauswahl.</p>
				<p>
					<strong>Quellen & verwendete Technologien:</strong> <br> <br>
					Wortschatz-Korpus der Universit√§t Leipzig<br> (<a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">wortschatz.uni-leipzig.de</a>)<br> TensorFlow
					und Keras ‚Äì f√ºr Modellarchitektur, Training und Export<br> (<a
						href="https://www.tensorflow.org/" target="_blank">tensorflow.org</a>)<br>

					TensorFlow.js ‚Äì f√ºr die Modellnutzung im Browser<br> (<a
						href="https://js.tensorflow.org" target="_blank">js.tensorflow.org</a>)<br>

					NumPy ‚Äì f√ºr Array-Manipulation im Trainingsprozess<br> (<a
						href="https://numpy.org" target="_blank">numpy.org</a>)<br>

					Regul√§re Ausdr√ºcke (re) ‚Äì zur Textvorverarbeitung in Python<br>
					(<a href="https://docs.python.org/3/library/re.html"
						target="_blank">python.org ‚Ä∫ re</a>)<br> Chart.js ‚Äì f√ºr
					Visualisierungen im Frontend (optional/erg√§nzend)<br> (<a
						href="https://www.chartjs.org" target="_blank">chartjs.org</a>)<br>

					HTML, CSS, JavaScript (ES6) ‚Äì f√ºr die Benutzeroberfl√§che,
					Eingabelogik und Eventsteuerung<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web"
						target="_blank">MDN Web Docs</a>)<br> Fetch API ‚Äì zum Laden
					des Tokenizer-Wortindex im Browser<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"
						target="_blank">MDN Fetch API</a>)<br> Google Fonts ‚Äì f√ºr
					typografische Gestaltung<br> (<a
						href="https://fonts.google.com" target="_blank">fonts.google.com</a>)<br>

					Vorlesungsfolien, √úbungen und Diskussionen im Rahmen der
					Lehrveranstaltung
				</p>

			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
