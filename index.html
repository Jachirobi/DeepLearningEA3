<!DOCTYPE html>
<html lang="de">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Language Model mit LSTM</title>
<link rel="stylesheet" href="style.css">
<script
	src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.13.0/dist/tf.min.js"></script>
<script
	src="https://cdnjs.cloudflare.com/ajax/libs/seedrandom/3.0.5/seedrandom.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>

	<div class="container">
		<header>
			<div class="header-inhalt">
				<h1>Language Model mit LSTM</h1>
				<button id="darkModeToggle" aria-pressed="false"
					aria-label="Dark Mode umschalten"
					title="Hell-/Dunkelmodus umschalten">🌙 Dark Mode
					aktivieren</button>
			</div>
		</header>

		<div class="allgemeine-informationen collapsible-section">
			<div class="section-header">
				<h2>Aufgabenstellung</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Erstellen Sie ein Language Model (LM) zur Wortvorhersage.
					Trainieren Sie dazu ein Long Short-Term Memory (LSTM) Netzwerk auf
					der Basis der Daten (siehe den Punkt „Daten“ unten) zur
					Wortvorhersage (Next Word Prediction). Mittels des trainierten LSTM
					Language-Models kann autoregressiv ein Text generiert werden, in
					dem das jeweils vorhergesagte Wort an den vorhandenen Text
					angehängt wird.</p>

				<h2>Modell und Optimierung</h2>
				<ul>
					<li>Stacked LSTM: 2 Hidden Layers (rekursiv) mit je 100 LSTM
						Units (andere Architekturen möglich).</li>
					<li>Softmax Output mit der Dimension des Dictionaries.</li>
					<li>Loss-Funktion: Cross-Entropy.</li>
					<li>Optimizer: Adam mit Lernrate 0.01.</li>
					<li>Batch-Size: 32 (Variation erlaubt).</li>
					<li>Anzahl Trainings-Epochen: nach Beobachtung des Loss, z. B.
						mit Tensorflow Visor.</li>
				</ul>

				<h2>Interaktion</h2>
				<ul>
					<li><strong>1)</strong> Der Nutzer kann einen Text (Prompt)
						eingeben. Dieser muss aus vollständigen, durch Leerzeichen
						getrennten Wörtern bestehen. Durch Klick auf den Button <em>„Vorhersage“</em>
						werden die wahrscheinlichsten folgenden Wörter mit
						Wahrscheinlichkeiten angezeigt. Der Nutzer kann eines dieser
						Wörter auswählen, welches dann angehängt wird. Danach startet
						automatisch eine neue Vorhersage.</li>
					<li><strong>2)</strong> Mit dem Button <em>„Weiter“</em> kann
						das wahrscheinlichste Wort automatisch angehängt werden. Danach
						startet erneut eine Vorhersage. Wiederholtes Klicken erzeugt einen
						fortlaufenden Text.</li>
					<li><strong>3)</strong> Über den Button <em>„Auto“</em> können
						automatisch bis zu 10 Wörter vorhergesagt werden.</li>
					<li><strong>4)</strong> Dieser Vorgang kann mit dem Button <em>„Stopp“</em>
						unterbrochen werden.</li>
					<li><strong>5)</strong> Mit dem Button <em>„Reset“</em> werden
						sowohl der eingegebene Text als auch das Netzwerk zurückgesetzt.</li>
				</ul>

				<h3>Buttons</h3>
				<ul>
					<li>Vorhersage</li>
					<li>Weiter</li>
					<li>Auto</li>
					<li>Stopp</li>
					<li>Reset</li>
				</ul>

				<h2>Experimente und Fragestellungen</h2>
				<ol>
					<li>Experimentieren Sie mit der Netzwerkarchitektur.
						Dokumentieren und begründen Sie Ihre finale Architektur.</li>
					<li>Notieren Sie, wie oft die Vorhersage exakt korrekt ist
						(k=1), und wie oft das korrekte nächste Wort unter den ersten k
						Vorhersagen liegt (k = 5, 10, 20, 100). Optional: Berechnung der
						Perplexity als zusätzliches Maß.</li>
					<li>Untersuchen Sie, ob sich die ursprünglichen Trainingsdaten
						mit dem trainierten Modell rekonstruieren lassen. Diskutieren Sie
						mögliche Datenschutzprobleme.</li>
				</ol>
			</div>
		</div>

		<section class="interaktion-section">
			<h2>Interaktive Wortvorhersage</h2>

			<div class="interaktion-ui">
				<label for="userPrompt">Eingabetext:</label>
				<textarea id="userPrompt"
					placeholder="Geben Sie hier Ihren Text ein..."></textarea>

				<div class="button-row">
					<button id="predictBtn">🔮 Vorhersage</button>
					<button id="weiterBtn">➡️ Weiter</button>
					<button id="autoBtn">🤖 Auto</button>
					<button id="stopBtn">🛑 Stopp</button>
					<button id="resetBtn">🔄 Reset</button>
				</div>
			</div>

			<div id="suggestionsContainer">
				<h3>Wortvorschläge</h3>
				<ul id="suggestionsList">
					<!-- Dynamisch eingefügte Vorschläge -->
				</ul>
			</div>

			<div id="chartContainer">
				<h3>Modellbewertung</h3>
				<canvas id="accuracyChart" width="400" height="200"></canvas>
			</div>
		</section>


		<section class="experimente collapsible-section">
			<div class="section-header">
				<h2>Experimente und Fragestellungen</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="experimente-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="experimente-section">
				<ol>
					<li><strong>Architektur:</strong><br>

						<p>Das Modell besteht aus einem Embedding-Layer mit einer
							Ausgabedimension von 64 und einer Eingabesequenzlänge von 5
							Token. Dies ermöglicht es dem Modell, sprachliche Abhängigkeiten
							über einen größeren Bereich zu erfassen.</p>

						<p>An das Embedding schließen sich 2 hidden Layer (in sich
							rekursiv) mit je 200 LSTM Units an. Tests mit mehr Layern haben
							gezeigt, dass der Loss sehr langsam gesunken ist und die Dauer
							des Trainings deutlich länger wurde. Die Erhöhung der Units hat
							zu einer Verbesserung des Trainingsergebnisses geführt.</p>
						<p>
							Zur Vermeidung von Overfitting wurde Dropout mit einer Rate von
							0,3 sowie ein recurrent Dropout von 0,2 in beiden LSTM-Schichten
							eingesetzt. Die erste LSTM-Schicht gibt eine vollständige
							Ausgabesequenz weiter (
							<code>return_sequences=True</code>
							), während die zweite nur den letzten Hidden State extrahiert, um
							eine kompakte Repräsentation der Eingabesequenz zu erzeugen.
						</p>

						<p>Die Ausgabe erfolgt über eine dichte Softmax-Schicht, deren
							Dimension der Größe des verwendeten Vokabulars entspricht (10.000
							Tokens). Diese Schicht gibt eine Wahrscheinlichkeitsverteilung
							über alle potenziell nächsten Wörter zurück.</p>

						<p>Für das Training wurde der Adam-Optimierer mit einer
							Lernrate von 0,001 verwendet, da sich dieser in Kombination mit
							LSTM-Netzen als robust und stabil erwiesen hat. Als Loss-Funktion
							wurde kategorische Kreuzentropie verwendet.</p>
						<p>Das Modell wurde mit einer Batch-Größe von 64 über 60
							Epochen hinweg trainiert. Die Anzahl der Epochen hat sich als am
							besten herausgestellt, weil loss und accuracy am dichtesten
							beieinander waren.</p>

						<p>Diese Architektur ermöglicht eine stabile, regulierte
							Modellierung sprachlicher Abhängigkeiten mit verbesserter
							Generalisierungsfähigkeit.</p>
					<li><strong>Trefferquote bei Vorhersagen:</strong><br>
						Die folgende Tabelle zeigt, wie oft das richtige Wort unter den
						Top-k-Vorhersagen lag:
						<ul>
							<li>k=1: <span id="acc1">...</span> %
							</li>
							<li>k=5: <span id="acc5">...</span> %
							</li>
							<li>k=10: <span id="acc10">...</span> %
							</li>
							<li>k=20: <span id="acc20">...</span> %
							</li>
							<li>k=100: <span id="acc100">...</span> %
							</li>
						</ul> Optional: Perplexity = <span id="perplexity">...</span></li>
					<li><strong>Rekonstruktion der Trainingsdaten:</strong><br>
						<p>Das trainierte LSTM-Sprachmodell speichert keine konkreten
							Trainingssätze, sondern lediglich statistische Muster in seinen
							Gewichtungen. Eine direkte Rekonstruktion der ursprünglichen
							Trainingsdaten ist daher nicht möglich.</p>

						<p>Allerdings kann es bei häufigen oder prägnanten
							Formulierungen, insbesondere bei kleinem Datensatz oder
							Overfitting, vorkommen, dass Teile des Trainingsmaterials
							sinngemäß oder sogar wörtlich reproduziert werden. In solchen
							Fällen besteht ein gewisses Datenschutzrisiko, insbesondere wenn
							personenbezogene Daten im Training enthalten waren. Auch wenn das
							Risiko bei einem Modell mit begrenztem Vokabular (z. B. 5000
							Wörter) und normalisierten Daten gering ist, sollte man bei der
							Veröffentlichung des Modells auf mögliche Rückschlüsse auf
							sensible Inhalte achten.</p></li>
				</ol>
			</div>
		</section>


		<div class="diskussion collapsible-section">
			<div class="section-header">
				<h2>Diskussion</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<p>Für die Regression der gegebenen Funktion wurde ein
					Feedforward Neural Network (FFNN) mit konfigurierbarer Architektur
					eingesetzt (2 Hidden-Layer, je 100 Neuronen, ReLU-Aktivierung,
					Adam-Optimizer mit Lernrate 0.01 und Batch-Size 32). Die Anzahl der
					Datenpunkte (N) und die Rauschvarianz (V) wurden gemäß Vorgabe
					flexibel gestaltet (hier typischerweise N = 100, V = 0.05). Dadurch
					konnten gezielt Auswirkungen von Datenmenge und Rauschstärke auf
					das Modellverhalten untersucht werden.</p>
				<p>
					Auf den <strong>unverrauschten Daten</strong> zeigte das Modell bei
					einer Epochenzahl von 1000 ein sehr gutes Fit-Verhalten (Train- und
					Test-MSE < 0.0001). Bei den <strong>verrauschten Daten</strong>
					führte ein Bereich von ca. 100-300 Epochen zu einer ausgewogenen
					Generalisierung (Train-MSE ~0.04-0.06, Test-MSE ~0.06-0.08). Bei
					sehr hoher Epochenanzahl (ab ca. 15.000) konnte Overfitting
					deutlich beobachtet werden: der Train-Loss sinkt weiter (&lt;0.02),
					während der Test-Loss ansteigt (&gt;0.12 bei 15.000 und &gt;0.32
					bei 20.000).
				</p>
				<p>Änderungen an der Architektur (z.B. zusätzliche Layer oder
					mehr Neuronen) zeigten, dass komplexere Modelle tendenziell
					schneller zu Overfitting neigen, insbesondere bei kleinen
					Datensätzen oder hoher Rauschvarianz. Ebenso wirkt sich die Wahl
					der Aktivierungsfunktion auf die Lernstabilität aus (ReLU erwies
					sich hier als robust). Die Erhöhung der Datenpunkte führte hingegen
					zu einem schnelleren Best-Fit Verhalten und einer Angleichung
					zwischen Test- und Trainings-MSE sowie einem viel späteren
					Overt-Fit.</p>
				<p>Insgesamt hat dieses Experiment sehr anschaulich
					verdeutlicht, wie Modellkomplexität, Trainingsdauer, Datenmenge und
					Rauschlevel gemeinsam das Lern- und Generalisierungsverhalten eines
					neuronalen Netzes bestimmen.</p>
			</div>
		</div>

		<main class="collapsible-section">
			<div class="section-header">
				<h2>Dokumentation</h2>
				<button class="toggle-button" aria-expanded="true"
					aria-controls="aufgaben-section"
					title="Abschnitt ein- oder ausklappen">⬇️ Einklappen</button>
			</div>
			<div class="section-content" id="aufgaben-section">
				<h3>1) Technisch</h3>
				<p>In der Lösung wurden folgende Frameworks und Libraries
					verwendet:</p>
				<ul>
					<li><strong>TensorFlow / Keras</strong>: Zum Erstellen,
						Trainieren und Speichern des LSTM-basierten Language Models
						(Sequential, LSTM, Embedding, Adam, EarlyStopping, etc.).</li>
					<li><strong>TensorFlow.js</strong>: Zum Konvertieren des
						Keras-Modells in ein browserkompatibles Format für die
						clientseitige Vorhersage.</li>
					<li><strong>NumPy</strong>: Für die effiziente Verarbeitung
						von Trainingsdaten (z. B. Sequenzen, Labels, Arrays).</li>
					<li><strong>re (Regular Expressions)</strong>: Zum Aufbereiten
						und Tokenisieren des Textkorpus (z. B. Satzzeichen trennen).</li>
					<li><strong>HTML, CSS, JavaScript</strong>: Zum Aufbau der
						Web-Oberfläche inklusive Texteingabe, Vorhersageanzeige und
						Interaktion mit dem TensorFlow.js-Modell im Browser.</li>
				</ul>
				<p>Das Modell nutzt ein gestapeltes LSTM mit Dropout und
					recurrent_dropout, um Overfitting zu vermeiden. Die Trainingsdaten
					werden mithilfe eines festen Werts von 5 Wörtern verarbeitet. Das
					Modell wird mit EarlyStopping trainiert und in .keras und .tfjs
					exportiert. Im Frontend wird durch Padding und Begrenzung der
					Wortindizes sichergestellt, dass Eingabesequenzen modellkompatibel
					bleiben. Im Testskript wird zusätzlich die Top-k-Genauigkeit und
					Perplexity als Evaluationsmaß berechnet.</p>

				<h3>2) Fachlich</h3>
				<p>
					Die vorliegende Anwendung basiert auf einem autoregressiven Modell
					zur <strong>nächsten Wortvorhersage</strong>, das mithilfe eines
					LSTM-Netzwerks realisiert wurde. Grundlage für das Training bildet
					ein deutschsprachiger Korpus, der aus dem <a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">Wortschatz-Korpus der Universität Leipzig</a>
					stammt. Der Text wurde im Vorfeld bereinigt und tokenisiert, wobei
					insbesondere Satzzeichen separiert und Groß-/Kleinschreibung
					erhalten blieben.
				</p>

				<p>
					Das Modell verarbeitet Wortsequenzen mit einer Länge von fünf
					Tokens und sagt jeweils das darauffolgende Wort vorher. Es wurde
					mit zwei gestapelten LSTM-Schichten (je 200 Einheiten) trainiert
					und durch Dropout regularisiert. Während des Trainings wurde
					<code>EarlyStopping</code>
					verwendet, um Überanpassung zu vermeiden. Als Optimierer kam
					<code>Adam</code>
					mit einer Lernrate von 0,001 zum Einsatz.
				</p>

				<p>Nach erfolgreichem Training wurde das Modell ins
					TensorFlow.js-Format konvertiert und in eine interaktive
					Weboberfläche integriert. Diese erlaubt es Nutzer:innen, eigene
					Texte einzugeben, automatische Wortvorschläge zu generieren oder
					Schritt für Schritt Texte fortzusetzen. Die Vorschläge basieren auf
					den wahrscheinlichsten nächsten Wörtern laut Modell – inklusive
					Prozentwerten.</p>

				<p>
					Zur qualitativen Bewertung des Modells wurden neben der klassischen
					Genauigkeit auch <em>Top-k-Genauigkeiten</em> (k=1, 5, 10, 20, 100)
					und die <strong>Perplexity</strong> als Kennzahlen berechnet. Diese
					geben Aufschluss über die Fähigkeit des Modells, plausible
					Folgewörter im Sprachkontext zu wählen.
				</p>
				<p>
					<strong>Quellen & verwendete Technologien:</strong><br>
					Wortschatz-Korpus der Universität Leipzig<br> (<a
						href="https://wortschatz.uni-leipzig.de/en/download/German?utm_source=chatgpt.com"
						target="_blank">wortschatz.uni-leipzig.de</a>)<br> TensorFlow
					und Keras – für Modellarchitektur, Training und Export<br> (<a
						href="https://www.tensorflow.org/" target="_blank">tensorflow.org</a>)<br>

					TensorFlow.js – für die Modellnutzung im Browser<br> (<a
						href="https://js.tensorflow.org" target="_blank">js.tensorflow.org</a>)<br>

					NumPy – für Array-Manipulation im Trainingsprozess<br> (<a
						href="https://numpy.org" target="_blank">numpy.org</a>)<br>

					Reguläre Ausdrücke (re) – zur Textvorverarbeitung in Python<br>
					(<a href="https://docs.python.org/3/library/re.html"
						target="_blank">python.org › re</a>)<br> Chart.js – für
					Visualisierungen im Frontend (optional/ergänzend)<br> (<a
						href="https://www.chartjs.org" target="_blank">chartjs.org</a>)<br>

					HTML, CSS, JavaScript (ES6) – für die Benutzeroberfläche,
					Eingabelogik und Eventsteuerung<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web"
						target="_blank">MDN Web Docs</a>)<br> Fetch API – zum Laden
					des Tokenizer-Wortindex im Browser<br> (<a
						href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"
						target="_blank">MDN Fetch API</a>)<br> Google Fonts – für
					typografische Gestaltung<br> (<a
						href="https://fonts.google.com" target="_blank">fonts.google.com</a>)<br>

					Vorlesungsfolien, Übungen und Diskussionen im Rahmen der
					Lehrveranstaltung
				</p>

			</div>
		</main>
		<footer>Thomas Brehmer</footer>
	</div>

	<script src="script2.js"></script>
</body>
</html>
